（没功夫认真写 readme.md，所以偷偷在项目内部建立个 readme.txt 简单记录一下）
本项目是用来生成 neo4j 建库所需的 csv 文件的，包括节点和关系
所有的生成类都基于 base_generator 类，减少了不少工作
所有的类传入的变量为文件夹路径，
    并且因为论文和成果的源数据结构不同，所以每个类传入的文件夹的类型是定死的
    比如 node_keyword_achievement 代表是抽取 关键词 节点，从成果文件中抽取
    值得注意的是，很多类没有后缀，如 node_author，抽取作者节点，
    没有后缀默认从论文库中抽取（少数几个是从作者信息表中抽取的，如论文作者的单位，论文作者的研究领域）
    默认从论文中抽取是因为，之前没考虑到论文和成果数据结构差别这么大，所以一开始的从论文中抽取的代码都没后缀
    同时，博硕和期刊中的论文，这里归一了，字段合并，都归为论文类（加了个 type 字段区分），可以减少代码工作量

博硕的数据不规整，有的地方作者没有code，关于author的一些代码暂时用try跳过
论文中的专家的研究领域，是从论文页面的专题页面抽取的，但或许应该从专家详情的「关注领域」字段抽取？
成果的专家没有链接，所以不能获取成果中专家的详细信息，这个很难受，所以成果的处理方式和论文完全不一样，
  所以我决定现在不处理成果，等后面实验室消歧全弄好再说吧
论文所属单位，通过论文中的专家，以及专家所属的单位这两个关系联合查询
主入口为 main.py



这里记录一下项目遇到的数据上的难点吧：
1. 知网的爬虫限制，爬多少页就不能爬了，要细分领域
2. 论文页面的作者的单位精确到某某学院，所以要用作者详情页的单位
3. 成果库的人的链接是空缺的，也就是查询不到详情；即使是论文，有链接的，有部分链接也是打不开的
4. 重名问题，现在是根据知网分配的唯一code来判断是否重名，但有很多人没有链接。有个办法是根据百度学术，
    然后对数据进行清晰消歧对齐，后面我在实验室会做这些东西，但是这个项目确实还有很多内容要做，
    要构建数据库，做后端，以及前端
5.